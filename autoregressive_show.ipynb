{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "「taming-transformers.ipynb」的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.13 64-bit ('setting-c': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.6.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "676b9487ebc1017606ab69df72f7b1d36db2f4478d7961a29e7f8d30d5947005"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Taming Transformers\n",
        "\n",
        "This notebook is a minimal working example to generate landscape images as in [Taming Transformers for High-Resolution Image Synthesis](https://github.com/CompVis/taming-transformers). **tl;dr** We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer."
      ],
      "metadata": {
        "id": "2U0NA9HrrZey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we install minimal required dependencies."
      ],
      "metadata": {
        "id": "eeBqWgMQDjZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model\n",
        "\n",
        "We load and print the config."
      ],
      "metadata": {
        "id": "5gaAQZXTxFxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "import argparse, os, sys, datetime, glob, importlib\n",
        "from omegaconf import OmegaConf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from dataset import dataset_combine, dataset_unpair, dataset_single\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "from taming_comb.modules.style_encoder.network import *\n",
        "from taming_comb.modules.diffusionmodules.model import * \n",
        "\n",
        "from taming_comb.models.cond_transformer import * \n",
        "\n",
        "import argparse\n",
        "\n",
        "def get_obj_from_str(string, reload=False):\n",
        "    module, cls = string.rsplit(\".\", 1)\n",
        "    if reload:\n",
        "        module_imp = importlib.import_module(module)\n",
        "        importlib.reload(module_imp)\n",
        "    return getattr(importlib.import_module(module, package=None), cls)\n",
        "\n",
        "\n",
        "\n",
        "def instantiate_from_config(config):\n",
        "    if not \"target\" in config:\n",
        "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
        "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "print('device: ', device)\n",
        "batch_size = 1 # 128\n",
        "learning_rate = 1e-5       # 256/512 lr=4.5e-6 from 71 epochs\n",
        "ne = 512  # Enlarge\n",
        "ed = 512\n",
        "img_size = 256\n",
        "\n",
        "switch_weight = 0.1 # self-reconstruction : a2b/b2a = 10 : 1\n",
        "\n",
        "dataset = 'summer2winter'\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "first_model_save_path = '{}_{}_{}_settingc_{}'.format(dataset, ed, ne, img_size)    # first stage model dir\n",
        "save_path = dataset + '{}_{}_{}_transformer'.format(dataset, ed, ne)    # second stage model dir\n",
        "print(save_path)\n",
        "root = '/home/jenny870207/data/' + dataset + '/'\n",
        "\n",
        "# load data\n",
        "train_data = dataset_unpair(root, 'train', 'A', 'B', img_size, img_size)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "# load first stage model\n",
        "'''f = os.path.join(os.getcwd(), first_model_save_path, 'settingc_latest.pt')\n",
        "config = OmegaConf.load('config_comb.yaml')\n",
        "config.model.params.embed_dim = args.ed\n",
        "config.model.params.n_embed = args.ne\n",
        "config.model.z_channels = args.z_channel\n",
        "config.model.resolution = 256\n",
        "first_model = instantiate_from_config(config.model)\n",
        "if(os.path.isfile(f)):\n",
        "    print('load ' + f)\n",
        "    ck = torch.load(f, map_location=device)\n",
        "    first_model.load_state_dict(ck['model_state_dict'], strict=False)\n",
        "first_model = first_model.to(device)\n",
        "first_model.eval()'''\n",
        "\n",
        "# load second stage model\n",
        "f = os.path.join(os.getcwd(), save_path, 'latest.pt')\n",
        "\n",
        "transformer_config = OmegaConf.load('transformer.yaml')\n",
        "transformer_config.model.params.first_stage_model_config.params.embed_dim = ed\n",
        "transformer_config.model.params.first_stage_model_config.params.n_embed = ne\n",
        "transformer_config.model.params.first_stage_model_config.z_channels = 256\n",
        "transformer_config.model.params.first_stage_model_config.resolution = 256\n",
        "transformer_config.model.params.f_path = os.path.join(os.getcwd(), first_model_save_path, 'settingc_latest.pt')\n",
        "transformer_config.model.params.device = str(device)\n",
        "model = instantiate_from_config(transformer_config.model)\n",
        "\n",
        "if(os.path.isfile(f)):\n",
        "    print('load ' + f)\n",
        "    ck = torch.load(f, map_location=device)\n",
        "    model.load_state_dict(ck['model_state_dict'], strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda\n",
            "summer2wintersummer2winter_512_512_transformer\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "load /home/jenny870207/VQI2I-setting-c/summer2winter_512_512_settingc_256/settingc_latest.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.75 GiB total capacity; 4.73 GiB already allocated; 2.75 MiB free; 4.97 GiB reserved in total by PyTorch)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-576f3a039a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mtransformer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_model_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'settingc_latest.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mtransformer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-576f3a039a11>\u001b[0m in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m\"target\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected key `target` to instantiate.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/VQI2I-setting-c/taming_comb/models/cond_transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transformer_config, first_stage_model_config, cond_stage_config, f_path, device, permuter_config, ckpt_path, ignore_keys, first_stage_key, cond_stage_key, downsample_cond_size, pkeep)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_first_stage_from_ckpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_stage_model_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_cond_stage_from_ckpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_stage_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpermuter_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/VQI2I-setting-c/taming_comb/models/cond_transformer.py\u001b[0m in \u001b[0;36minit_first_stage_from_ckpt\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_string_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/jenny870207/.local/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.75 GiB total capacity; 4.73 GiB already allocated; 2.75 MiB free; 4.97 GiB reserved in total by PyTorch)"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUOUJaTj02Bq",
        "outputId": "6671562d-f7f6-4407-ee40-22e1b83421e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the model."
      ],
      "metadata": {
        "id": "FzQqNgiLEJ9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the checkpoint."
      ],
      "metadata": {
        "id": "njAiY_aqENwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load example data\n",
        "\n",
        "Load an example segmentation and visualize."
      ],
      "metadata": {
        "id": "tTusbqk2y0u3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "'''img_path = \"/home/jenny870207/data/summer2winter/trainB/2005-06-26 14:04:52.jpg\"\n",
        "img = Image.open(img_path)\n",
        "display(img)\n",
        "img = np.array(img)\n",
        "print(img.shape)\n",
        "img = np.eye(256)[img]\n",
        "img = torch.tensor(img.transpose(2,0,1)[None]).to(dtype=torch.float32, device=model.device)'''\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "8LiAOU6C-vTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize"
      ],
      "metadata": {
        "id": "oCNy13FGPMy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model also employs a VQGAN for the conditioning information, i.e. the segmentation in this example. Let's autoencode the segmentation map. Encoding returns both the quantized code and its representation in terms of indices of a learned codebook."
      ],
      "metadata": {
        "id": "hNBdHGbNTrfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's sample indices corresponding to codes from the image VQGAN given the segmentation code. We init randomly and take a look."
      ],
      "metadata": {
        "id": "pp-OsC1RXAQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def show_image(s):\n",
        "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0]\n",
        "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
        "  s = Image.fromarray(s)\n",
        "  display(s)\n",
        "\n",
        "'''codebook_size = config.model.params.first_stage_model_config.params.embed_dim\n",
        "#z_indices_shape = #c_indices.shape\n",
        "#z_code_shape = #c_code.shape\n",
        "z_indices = torch.randint(codebook_size, 16*16, device=model.device)\n",
        "x_sample = model.decode_to_img(z_indices, z_code_shape)\n",
        "show_image(x_sample)'''"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "VTfao3jJSCfW",
        "outputId": "1a3320ff-a389-4759-ced3-5dee58bf7a29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample an image\n",
        "\n",
        "We use the transformer in a sliding window manner to sample all code entries sequentially. The code below assumes a window size of $16\\times 16$."
      ],
      "metadata": {
        "id": "ftDWneY3zJZ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
        "\n",
        "    Parameters:\n",
        "        input_image (tensor) --  the input image tensor array\n",
        "        imtype (type)        --  the desired type of the converted numpy array\n",
        "    \"\"\"\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor[0].clamp(-1.0, 1.0).cpu().float().numpy()  # convert it into a numpy array\n",
        "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
        "    else:  # if it is a numpy array, do nothing\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "root = '/home/jenny870207/data/{}'.format(dataset)\n",
        "validation_data = dataset_single(root, 'train', 'A', img_size, img_size, flip=False)\n",
        "test_loader = DataLoader(validation_data, batch_size=1, shuffle=True, pin_memory=True)\n",
        "img = next(iter(test_loader))\n",
        "img = img.to(device)\n",
        "#print(img.shape)#\n",
        "\n",
        "original_size = 16\n",
        "size = 20\n",
        "codebook_size = 512\n",
        "window_size = 16\n",
        "\n",
        "img_size = 16*size\n",
        "\n",
        "#int(window_size/2)\n",
        "\n",
        "z_code, z_indices, style = model.encode_to_z(img, 1)\n",
        "z_code_shape = (1, codebook_size, size, size)#z_code.shape\n",
        "#print('z_code_shape: ', z_code_shape)\n",
        "z_indices_shape = z_indices.shape\n",
        "#print(z_code.shape)\n",
        "\n",
        "style = model.first_stage_model.encode_style(img, 1)\n",
        "\n",
        "\n",
        "\n",
        "z_random = torch.randint(codebook_size, (size*size,)).to(device)\n",
        "z_random = z_random.reshape(z_code_shape[0], size, size)\n",
        "z_random[:, :original_size, :original_size] = z_indices.reshape(z_code_shape[0], original_size, original_size)\n",
        "\n",
        "idx = z_random\n",
        "#idx = z_indices\n",
        "idx = idx.reshape(z_code_shape[0],size,size)\n",
        "\n",
        "c_size = 256\n",
        "coordinate = np.arange(c_size*c_size).reshape(c_size,c_size,1)/(c_size*c_size)\n",
        "coordinate = torch.from_numpy(coordinate)\n",
        "c = model.get_c(coordinate)\n",
        "#c = torch.squeeze(c, 1)\n",
        "cidx = c.to(device)\n",
        "_, cidx = model.encode_to_c(cidx)\n",
        "cidx = cidx.reshape(z_code_shape[0],16,16)\n",
        "\n",
        "temperature = 1.0\n",
        "top_k = None#100\n",
        "update_every = 50\n",
        "\n",
        "start_t = time.time()\n",
        "for i in range(0, z_code_shape[2]-0):\n",
        "#for i in range(original_size, z_code_shape[2]-0):\n",
        "  if i <= int(window_size/2):\n",
        "    local_i = i\n",
        "  elif z_code_shape[2]-i < int(window_size/2):\n",
        "    local_i = window_size-(z_code_shape[2]-i)\n",
        "  else:\n",
        "    local_i = int(window_size/2)\n",
        "  for j in range(0,z_code_shape[3]-0):\n",
        "  #for j in range(original_size,z_code_shape[3]-0):\n",
        "    if j <= int(window_size/2):\n",
        "      local_j = j\n",
        "    elif z_code_shape[3]-j < int(window_size/2):\n",
        "      local_j = window_size-(z_code_shape[3]-j)\n",
        "    else:\n",
        "      local_j = int(window_size/2)\n",
        "\n",
        "\n",
        "\n",
        "    i_start = i-local_i\n",
        "    i_end = i_start + window_size\n",
        "    '''if i_end >= original_size:\n",
        "      i_end = i\n",
        "      i_start = i-2*local_i'''\n",
        "\n",
        "    j_start = j-local_j\n",
        "    j_end = j_start + window_size\n",
        "    '''if j_end >= original_size:\n",
        "      j_end = j\n",
        "      j_start = j-2*local_j'''\n",
        "    \n",
        "    patch = idx[:,i_start:i_end,j_start:j_end]\n",
        "    patch = patch.reshape(patch.shape[0],-1)\n",
        "    #cpatch = cidx[:, i_start:i_end, j_start:j_end]\n",
        "    cpatch = cidx\n",
        "    cpatch = cpatch.reshape(cpatch.shape[0], -1)\n",
        "\n",
        "    czpatch = torch.cat((cpatch, patch), dim=1)\n",
        "    logits,_ = model.transformer(czpatch[:,:-1]) #[:,:-1]\n",
        "  \n",
        "\n",
        "    #print(logits.shape)\n",
        "    logits = logits[:, -window_size*window_size:, :]\n",
        "    logits = logits.reshape(z_code_shape[0],window_size,window_size,-1)\n",
        "    #print(logits.shape)\n",
        "    logits = logits[:,local_i,local_j,:]\n",
        "    #print(logits.shape)\n",
        "\n",
        "    logits = logits/temperature\n",
        "\n",
        "    if top_k is not None:\n",
        "      logits = model.top_k_logits(logits, top_k)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    print('original idx: ', idx[:,i,j])\n",
        "\n",
        "    idx[:,i,j] = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    print('new idx: ', idx[:,i,j])\n",
        " \n",
        "  \n",
        "    '''step = i*z_code_shape[3]+j\n",
        "    if step%update_every==0 or step==z_code_shape[2]*z_code_shape[3]-1:\n",
        "\n",
        "\n",
        "      x_sample = model.decode_to_img(idx, z_code_shape, style, 0)\n",
        "      #clear_output()\n",
        "      print(f\"Time: {time.time() - start_t} seconds\")\n",
        "      print(f\"Step: ({i},{j}) | Local: ({local_i},{local_j}) | Crop: ({i_start}:{i_end},{j_start}:{j_end})\")\n",
        "\n",
        "      show_image(x_sample)'''\n",
        "#clear_output()\n",
        "x_sample = model.decode_to_img(idx, (1, codebook_size, size, size), style, 1)\n",
        "show_image(x_sample)\n",
        "print(x_sample.shape)\n",
        "plt.imshow(tensor2im(img))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "3Pu-VjGHnFta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "z_random = torch.randint(codebook_size, (size*size,)).to(device)\n",
        "z_random = z_random.reshape(z_code_shape[0], size, size)\n",
        "z_random[:, :original_size, :original_size] = z_indices.reshape(z_code_shape[0], original_size, original_size)\n",
        "\n",
        "\n",
        "x_sample = model.decode_to_img(z_random, (1, codebook_size, size, size), style, 1)\n",
        "show_image(x_sample)\n",
        "print(x_sample.shape)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}